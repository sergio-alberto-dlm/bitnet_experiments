{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import re\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seeds\n",
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True  # Ensure reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "args = Namespace(\n",
    "    emb_size=200,\n",
    "    num_layers=5,\n",
    "    hidden_size=50,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=30,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    batch_size=16,\n",
    "    lr=3e-3,\n",
    "    num_epochs=100,\n",
    "    patience=10,\n",
    "    lr_patience=10,\n",
    "    lr_factor=0.5,\n",
    "    savedir='model_rnn'\n",
    ")\n",
    "os.makedirs(args.savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path, sep='\\r\\n', engine='python', header=None).loc[:, 0].values.tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)\n",
    "    tweet = re.sub(r'#\\S+', '', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'\\W', ' ', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "X_train = [preprocess_tweet(tweet) for tweet in load_data('./data_mex20/mex20_train.txt')]\n",
    "X_val = [preprocess_tweet(tweet) for tweet in load_data('./data_mex20/mex20_val.txt')]\n",
    "y_train = np.array(load_data('./data_mex20/mex20_train_labels.txt')).reshape(-1)\n",
    "y_val = np.array(load_data('./data_mex20/mex20_val_labels.txt')).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, add_special_tokens=True, return_tensors='pt',\n",
    "                                   truncation=True, max_length=max_length, padding='max_length',\n",
    "                                   return_attention_mask=True)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Create datasets    \n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer, args.max_seq_len)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer, args.max_seq_len)\n",
    "\n",
    "# Create dataloaders \n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNNLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Linear layers for input and hidden state\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Activation function\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.tanh(self.i2h(combined))\n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.emb_size)\n",
    "        self.rnn = SimpleRNNLayer(args.emb_size, args.hidden_size)\n",
    "        self.fc = nn.Linear(args.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        batch_size, seq_len, _ = embedded.size()\n",
    "        \n",
    "        hidden = self.rnn.init_hidden(batch_size)\n",
    "        for i in range(seq_len):\n",
    "            output, hidden = self.rnn(embedded[:, i, :], hidden)\n",
    "        \n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 330/330 [00:23<00:00, 14.07it/s, train_accuracy=0.709, train_loss=0.604]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.6040, Train Acc: 0.7088, Val Loss: 0.0380, Val Acc: 0.7121, Time: 24.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 330/330 [00:25<00:00, 13.14it/s, train_accuracy=0.716, train_loss=0.597]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 0.5967, Train Acc: 0.7156, Val Loss: 0.0381, Val Acc: 0.7019, Time: 49.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 330/330 [00:25<00:00, 13.07it/s, train_accuracy=0.73, train_loss=0.581]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 0.5812, Train Acc: 0.7302, Val Loss: 0.0384, Val Acc: 0.7053, Time: 75.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 330/330 [00:25<00:00, 13.15it/s, train_accuracy=0.746, train_loss=0.561]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 0.5607, Train Acc: 0.7463, Val Loss: 0.0392, Val Acc: 0.7019, Time: 100.88s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 330/330 [00:25<00:00, 12.88it/s, train_accuracy=0.754, train_loss=0.544]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 0.5442, Train Acc: 0.7545, Val Loss: 0.0421, Val Acc: 0.7002, Time: 126.95s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 330/330 [00:25<00:00, 13.14it/s, train_accuracy=0.759, train_loss=0.529]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Train Loss: 0.5289, Train Acc: 0.7592, Val Loss: 0.0436, Val Acc: 0.6985, Time: 152.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 330/330 [00:24<00:00, 13.49it/s, train_accuracy=0.766, train_loss=0.52]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Train Loss: 0.5197, Train Acc: 0.7662, Val Loss: 0.0449, Val Acc: 0.6968, Time: 177.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 330/330 [00:24<00:00, 13.41it/s, train_accuracy=0.769, train_loss=0.506]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Train Loss: 0.5061, Train Acc: 0.7694, Val Loss: 0.0458, Val Acc: 0.7002, Time: 202.47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 330/330 [00:37<00:00,  8.89it/s, train_accuracy=0.772, train_loss=0.499]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Train Loss: 0.4993, Train Acc: 0.7725, Val Loss: 0.0480, Val Acc: 0.7002, Time: 240.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 330/330 [00:26<00:00, 12.44it/s, train_accuracy=0.775, train_loss=0.49]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 0.4904, Train Acc: 0.7749, Val Loss: 0.0514, Val Acc: 0.6899, Time: 267.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 330/330 [00:26<00:00, 12.64it/s, train_accuracy=0.772, train_loss=0.501]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement. Breaking out of loop.\n",
      "Total Training Time: 293.62 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_preds(raw_logit):\n",
    "    return torch.sigmoid(raw_logit)\n",
    "\n",
    "def model_eval(model, data, device, loss_fn):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for item in data:\n",
    "            ids, labels = item['input_ids'].to(device), item['labels'].to(device)\n",
    "            outputs = get_preds(model(ids))\n",
    "            loss = loss_fn(outputs.view(-1), labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = (outputs.view(-1) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "    val_loss /= len(data.dataset)\n",
    "    accuracy = correct / len(data.dataset)\n",
    "    return val_loss, accuracy\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))\n",
    "\n",
    "model = TextClassifier(args).to(args.device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=args.lr_patience, factor=args.lr_factor)\n",
    "\n",
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "n_no_improve = 0\n",
    "train_loss_history, train_metric_history = [], []\n",
    "val_loss_history, val_metric_history = [], []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_loss_epoch, correct = 0, 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.num_epochs}\")\n",
    "    for item in loop:\n",
    "        ids, labels = item['input_ids'].to(args.device), item['labels'].to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = get_preds(model(ids))\n",
    "        loss = criterion(outputs.view(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "        preds = (outputs.view(-1) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        loop.set_postfix(train_loss=train_loss_epoch/len(train_loader), train_accuracy=correct/len(train_loader.dataset))\n",
    "\n",
    "    train_loss = train_loss_epoch / len(train_loader)\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_metric_history.append(train_accuracy)\n",
    "\n",
    "    val_loss, val_accuracy = model_eval(model, val_loader, args.device, criterion)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_metric_history.append(val_accuracy)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    is_improvement = val_accuracy > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = val_accuracy\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    save_checkpoint({'epoch': epoch + 1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), \n",
    "                     'scheduler': scheduler.state_dict(), 'best_metric': best_metric}, is_improvement, args.savedir)\n",
    "\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{args.num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Time: {time.time() - start_time:.2f}s')\n",
    "\n",
    "print(f\"Total Training Time: {time.time() - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_lr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
