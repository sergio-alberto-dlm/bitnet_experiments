{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import re\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seeds\n",
    "seed = 1111\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True  # Ensure reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')\n",
    "args = Namespace(\n",
    "    emb_size=200,\n",
    "    num_layers=5,\n",
    "    n_heads=5,\n",
    "    head_size=50,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=30,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    batch_size=16,\n",
    "    lr=3e-3,\n",
    "    num_epochs=100,\n",
    "    patience=10,\n",
    "    lr_patience=10,\n",
    "    lr_factor=0.5,\n",
    "    savedir='model'\n",
    ")\n",
    "os.makedirs(args.savedir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path, sep='\\r\\n', engine='python', header=None).loc[:, 0].values.tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return []\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)\n",
    "    tweet = re.sub(r'#\\S+', '', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'\\W', ' ', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "X_train = [preprocess_tweet(tweet) for tweet in load_data('./data_mex20/mex20_train.txt')]\n",
    "X_val = [preprocess_tweet(tweet) for tweet in load_data('./data_mex20/mex20_val.txt')]\n",
    "y_train = np.array(load_data('./data_mex20/mex20_train_labels.txt')).reshape(-1)\n",
    "y_val = np.array(load_data('./data_mex20/mex20_val_labels.txt')).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, add_special_tokens=True, return_tensors='pt',\n",
    "                                   truncation=True, max_length=max_length, padding='max_length',\n",
    "                                   return_attention_mask=True)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Create datasets    \n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer, args.max_seq_len)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer, args.max_seq_len)\n",
    "\n",
    "# Create dataloaders \n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, emb_size, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(emb_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_size, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        k, q, v = self.key(x), self.query(x), self.value(x)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * (k.shape[-1] ** -0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(1) == 0, float('-inf'))\n",
    "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
    "        return torch.matmul(attn, v)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Attention(emb_size, head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads * head_size, emb_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(x))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_size, 4 * emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * emb_size, emb_size),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, head_size, emb_size)\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.ff = FeedForward(emb_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.mha(self.ln1(x), mask)\n",
    "        return x + self.ff(self.ln2(x))\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.emb_size)\n",
    "        self.pos = nn.Embedding(args.max_seq_len, args.emb_size)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(args.emb_size, args.n_heads, args.head_size) for _ in range(args.num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(args.emb_size)\n",
    "        self.lm_head = nn.Linear(args.emb_size * args.max_seq_len, 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, mask=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.emb(idx) + self.pos(torch.arange(T, device=self.args.device))\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        x = self.ln_f(x)\n",
    "        x = x.view(B, -1)\n",
    "        return self.lm_head(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = next(iter(train_loader))\n",
    "# ids = item['input_ids']\n",
    "# mask = item['attention_mask']\n",
    "# labels = item['labels']\n",
    "# item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 330/330 [01:46<00:00,  3.10it/s, train_accuracy=0.698, train_loss=0.726]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.7263, Train Acc: 0.6980, Val Loss: 0.0413, Val Acc: 0.7240, Time: 108.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 330/330 [02:29<00:00,  2.21it/s, train_accuracy=0.816, train_loss=0.456]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 0.4563, Train Acc: 0.8158, Val Loss: 0.0364, Val Acc: 0.7530, Time: 260.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 330/330 [02:47<00:00,  1.96it/s, train_accuracy=0.873, train_loss=0.306]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 0.3064, Train Acc: 0.8727, Val Loss: 0.0275, Val Acc: 0.8058, Time: 431.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 330/330 [02:45<00:00,  1.99it/s, train_accuracy=0.907, train_loss=0.238]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 0.2379, Train Acc: 0.9072, Val Loss: 0.0290, Val Acc: 0.8058, Time: 599.27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 330/330 [02:56<00:00,  1.87it/s, train_accuracy=0.923, train_loss=0.183]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 0.1830, Train Acc: 0.9233, Val Loss: 0.0361, Val Acc: 0.7853, Time: 778.49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 330/330 [02:42<00:00,  2.03it/s, train_accuracy=0.948, train_loss=0.134]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Train Loss: 0.1342, Train Acc: 0.9483, Val Loss: 0.0420, Val Acc: 0.7888, Time: 943.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 330/330 [03:04<00:00,  1.79it/s, train_accuracy=0.951, train_loss=0.116]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Train Loss: 0.1165, Train Acc: 0.9509, Val Loss: 0.0425, Val Acc: 0.7922, Time: 1130.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 330/330 [02:43<00:00,  2.02it/s, train_accuracy=0.959, train_loss=0.109]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Train Loss: 0.1092, Train Acc: 0.9591, Val Loss: 0.0437, Val Acc: 0.7888, Time: 1295.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 330/330 [02:45<00:00,  1.99it/s, train_accuracy=0.968, train_loss=0.0865]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Train Loss: 0.0865, Train Acc: 0.9676, Val Loss: 0.0487, Val Acc: 0.7530, Time: 1463.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 330/330 [02:37<00:00,  2.10it/s, train_accuracy=0.973, train_loss=0.0714]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 0.0714, Train Acc: 0.9735, Val Loss: 0.0570, Val Acc: 0.7649, Time: 1623.57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 330/330 [02:39<00:00,  2.07it/s, train_accuracy=0.978, train_loss=0.059]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Train Loss: 0.0590, Train Acc: 0.9782, Val Loss: 0.0602, Val Acc: 0.7615, Time: 1785.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 330/330 [02:31<00:00,  2.18it/s, train_accuracy=0.973, train_loss=0.079]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Train Loss: 0.0790, Train Acc: 0.9729, Val Loss: 0.0538, Val Acc: 0.7683, Time: 1939.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 330/330 [25:12<00:00,  4.58s/it, train_accuracy=0.974, train_loss=0.0681]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement. Breaking out of loop.\n",
      "Total Training Time: 3454.96 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_preds(raw_logit):\n",
    "    return torch.sigmoid(raw_logit)\n",
    "\n",
    "def model_eval(model, data, device, loss_fn):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for item in data:\n",
    "            ids, mask, labels = item['input_ids'].to(device), item['attention_mask'].to(device), item['labels'].to(device)\n",
    "            outputs = get_preds(model(ids, mask))\n",
    "            loss = loss_fn(outputs.view(-1), labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = (outputs.view(-1) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "    val_loss /= len(data.dataset)\n",
    "    accuracy = correct / len(data.dataset)\n",
    "    return val_loss, accuracy\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, filename=\"checkpoint.pt\"):\n",
    "    filename = os.path.join(checkpoint_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(checkpoint_path, \"model_best.pt\"))\n",
    "\n",
    "model = Transformer(args).to(args.device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=args.lr_patience, factor=args.lr_factor)\n",
    "\n",
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "n_no_improve = 0\n",
    "train_loss_history, train_metric_history = [], []\n",
    "val_loss_history, val_metric_history = [], []\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    train_loss_epoch, correct = 0, 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.num_epochs}\")\n",
    "    for item in loop:\n",
    "        ids, mask, labels = item['input_ids'].to(args.device), item['attention_mask'].to(args.device), item['labels'].to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = get_preds(model(ids, mask))\n",
    "        loss = criterion(outputs.view(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "        preds = (outputs.view(-1) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        loop.set_postfix(train_loss=train_loss_epoch/len(train_loader), train_accuracy=correct/len(train_loader.dataset))\n",
    "\n",
    "    train_loss = train_loss_epoch / len(train_loader)\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_metric_history.append(train_accuracy)\n",
    "\n",
    "    val_loss, val_accuracy = model_eval(model, val_loader, args.device, criterion)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_metric_history.append(val_accuracy)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    is_improvement = val_accuracy > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = val_accuracy\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "\n",
    "    save_checkpoint({'epoch': epoch + 1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), \n",
    "                     'scheduler': scheduler.state_dict(), 'best_metric': best_metric}, is_improvement, args.savedir)\n",
    "\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of loop.\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{args.num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Time: {time.time() - start_time:.2f}s')\n",
    "\n",
    "print(f\"Total Training Time: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
