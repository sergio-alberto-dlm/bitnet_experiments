{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools \n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pytorch \n",
    "import torch \n",
    "# from bitnet import BitLinear # Binary layer \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Scikit learn\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "seed = 1111\n",
    "random.seed(seed) # python seed \n",
    "np.random.seed(seed) # numpy seed \n",
    "torch.manual_seed(seed) # torch seed \n",
    "torch.backends.cudnn.benchmark = False # Ensure reproducibility of CUDA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(emb_size=50, num_layers=2, n_heads=2, head_size=25, vocab_size=31002, max_seq_len=100, device='cpu', batch_size=16)\n"
     ]
    }
   ],
   "source": [
    "# Configuration \n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased') # pre-trained tokenizer\n",
    "args = Namespace()\n",
    "args.emb_size = 50\n",
    "args.num_layers = 2\n",
    "args.n_heads = 2\n",
    "args.head_size = 25\n",
    "args.vocab_size = tokenizer.vocab_size\n",
    "args.max_seq_len = 100\n",
    "args.device = 'cpu'\n",
    "args.batch_size = 16\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets \n",
    "X_train = pd.read_csv('./data_mex20/mex20_train.txt', sep='\\r\\n', engine='python', header=None).loc[:, 0].values.tolist()\n",
    "X_val = pd.read_csv('./data_mex20/mex20_val.txt', sep='\\r\\n', engine='python', header=None).loc[:, 0].values.tolist()\n",
    "# Labels \n",
    "y_train = np.array(pd.read_csv('./data_mex20/mex20_train_labels.txt', sep='\\r\\n', engine='python', header=None)).reshape(-1)\n",
    "y_val = np.array(pd.read_csv('./data_mex20/mex20_val_labels.txt', sep='\\r\\n', engine='python', header=None)).reshape(-1)\n",
    "\n",
    "# Preprocess data \n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)  # Eliminar URLs\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)     # Eliminar menciones\n",
    "    tweet = re.sub(r'#\\S+', '', tweet)     # Eliminar hashtags\n",
    "    tweet = tweet.lower()                  # Convertir a minúsculas\n",
    "    tweet = re.sub(r'\\W', ' ', tweet)      # Eliminar caracteres especiales\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()  # Eliminar espacios extra\n",
    "    return tweet\n",
    "\n",
    "for i, tweet in enumerate(X_train):\n",
    "    X_train[i] = preprocess_tweet(tweet)\n",
    "for i, tweet in enumerate(X_val):\n",
    "    X_val[i] = preprocess_tweet(tweet)\n",
    "    \n",
    "# print(X_train)\n",
    "# print(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        # self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            add_special_tokens=True,  # Agrega los tokens especiales '[CLS]' y '[SEP]'\n",
    "            return_tensors='pt',      # Retorna tensores de PyTorch\n",
    "            truncation=True,          # Trunca textos que superen la longitud máxima del modelo\n",
    "            max_length=max_length,    # Longitud máxima para los inputs\n",
    "            padding='max_length',     # Añade padding hasta 'max_length' si el tweet es más corto\n",
    "            return_attention_mask=True  # Retorna la máscara de atención\n",
    "            )\n",
    "        self.labels = labels \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Create datasets    \n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer, args.max_seq_len)\n",
    "val_dataset = TextDataset(X_val, y_val, tokenizer, args.max_seq_len)\n",
    "\n",
    "# Create dataloaders \n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, head_size, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(emb_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(emb_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_size, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # keys, queries, values \n",
    "        k = self.key(x)  # (B, T, hs)\n",
    "        q = self.query(x)  # (B, T, hs)\n",
    "        v = self.value(x)  # (B, T, hs)\n",
    "\n",
    "        # compute scores \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * (k.shape[-1] ** -0.5) # (B, T, hs) @ (B, hs, T) = (B, T, T)\n",
    "\n",
    "        # apply mask \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf')) \n",
    "\n",
    "        # normalize scores\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # weighted sum \n",
    "        out = torch.matmul(attn, v) # (B, T, T) @ (B, T, hs) = (B, T, hs)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi Head Attention block's Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, head_size, emb_size, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Attention(emb_size, head_size, max_seq_len) for _ in range(n_heads)])\n",
    "        self.proy = nn.Linear(n_heads * head_size, emb_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        x = self.dropout(self.proy(x))\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A feed forward layer: Linear + Relu \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_size, 4 * emb_size),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(4 * emb_size, emb_size), \n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class block(nn.Module):\n",
    "    \"\"\" Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, n_heads, head_size, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, head_size, emb_size, max_seq_len)\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "        self.ff = FeedForward(emb_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.mha(self.ln1(x), mask)\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\" Model for generation and classification \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.args = args\n",
    "        self.emb = nn.Embedding(args.vocab_size, args.emb_size)\n",
    "        self.pos = nn.Embedding(args.max_seq_len, args.emb_size)\n",
    "        self.ln_f = nn.LayerNorm(args.emb_size) # final layer norm \n",
    "        self.lm_head = nn.Linear(args.emb_size, 1) # final dense layer \n",
    "        self.layers = nn.Sequential(*[block(args.emb_size, args.n_heads, args.head_size, args.max_seq_len) for _ in range(args.num_layers)])\n",
    "\n",
    "        # initialize weights \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, mask):\n",
    "        B, T = idx.shape # idx is a chunk of sequences (B, T)\n",
    "        x = self.emb(idx) + self.pos(torch.arange(T, device=self.args.device))\n",
    "        x = self.layers(x, mask)\n",
    "        x = self.ln_f(x)\n",
    "        logit = self.lm_head(x) \n",
    "\n",
    "        return logit \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
